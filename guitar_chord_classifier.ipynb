{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuitarChordDetectModel:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.file_names = os.listdir(data_path)\n",
    "        self.output_images_path=\"data\\\\frame_images\"\n",
    "\n",
    "        for i in range(len(self.file_names)):\n",
    "            self.file_names[i] = os.path.join(self.data_path, self.file_names[i])\n",
    "        print(self.file_names)\n",
    "        print(f\"{len(self.file_names)} files\")\n",
    "\n",
    "    def play_video_file(self, video_file=None, save_frames=False, frame_save_interval=0.2):\n",
    "        # Set the first video as default video\n",
    "        if video_file is None:\n",
    "            video_file = self.file_names[0]\n",
    "\n",
    "        print(f\"video file: {video_file}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "        # Check if the video file opened successfully\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video file.\")\n",
    "            return\n",
    "        \n",
    "        # Get a unique video identifier from the file name\n",
    "        video_id = os.path.splitext(os.path.basename(video_file))[0]\n",
    "        self.output_images_path = os.path.join(self.output_images_path, video_id)\n",
    "        \n",
    "        # Initialize variables for frame saving\n",
    "        if save_frames:\n",
    "            os.makedirs(self.output_images_path, exist_ok=True)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
    "            frame_interval = int(fps * frame_save_interval)  # Interval in frames\n",
    "            saved_frame_count = 0\n",
    "            frame_count = 0\n",
    "\n",
    "        # Read and display frames in a loop\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # Read a frame from the video\n",
    "            if not ret:  # Break the loop if no frames are left\n",
    "                print(\"End of video.\")\n",
    "                break\n",
    "\n",
    "            # Display the current frame\n",
    "            cv2.imshow('Video', frame)\n",
    "\n",
    "            # Save frames if enabled\n",
    "            if save_frames and frame_count % frame_interval == 0:\n",
    "                output_path = os.path.join(self.output_images_path, f\"{video_id}_frame_{saved_frame_count:04d}.jpg\")\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                saved_frame_count += 1\n",
    "\n",
    "            # Increment frame counter\n",
    "            if save_frames:\n",
    "                frame_count += 1\n",
    "\n",
    "            # Break the loop when 'q' is pressed\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                print(\"Video playback interrupted by user.\")\n",
    "                break\n",
    "\n",
    "        # Release the video capture object and close all windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        if save_frames:\n",
    "            print(f\"Saved {saved_frame_count} frames to '{self.output_images_path}'.\")\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load training data\n",
    "        train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            directory=self.output_images_path,\n",
    "            image_size=(224, 224),  # Resize images\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # 20% for testing\n",
    "            subset=\"training\",     # Load the training subset\n",
    "            seed=123,              # Ensure reproducibility\n",
    "            label_mode='int'\n",
    "        )\n",
    "\n",
    "        # Load testing data\n",
    "        test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            directory=self.output_images_path,\n",
    "            image_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # 20% for testing\n",
    "            subset=\"validation\",   # Load the validation subset\n",
    "            seed=123,\n",
    "            label_mode='int'\n",
    "        )\n",
    "\n",
    "        # Display class names\n",
    "        print(train_dataset.class_names)\n",
    "        return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\user\\\\Videos\\\\Computer-Vision_Data\\\\Guitar-Chords\\\\A-Chord.mp4', 'C:\\\\Users\\\\user\\\\Videos\\\\Computer-Vision_Data\\\\Guitar-Chords\\\\Bm-Chord.mp4', 'C:\\\\Users\\\\user\\\\Videos\\\\Computer-Vision_Data\\\\Guitar-Chords\\\\C-Chord.mp4', 'C:\\\\Users\\\\user\\\\Videos\\\\Computer-Vision_Data\\\\Guitar-Chords\\\\D-Chord.mp4', 'C:\\\\Users\\\\user\\\\Videos\\\\Computer-Vision_Data\\\\Guitar-Chords\\\\G-Chord.mp4']\n",
      "5 files\n"
     ]
    }
   ],
   "source": [
    "guitarChordDetectModel = GuitarChordDetectModel(rf'C:\\Users\\user\\Videos\\Computer-Vision_Data\\Guitar-Chords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video file: C:\\Users\\user\\Videos\\Computer-Vision_Data\\Guitar-Chords\\G-Chord.mp4\n",
      "End of video.\n",
      "Saved 148 frames to 'data\\frame_images\\G-Chord'.\n"
     ]
    }
   ],
   "source": [
    "guitarChordDetectModel.play_video_file(guitarChordDetectModel.file_names[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 489 files belonging to 5 classes.\n",
      "Using 392 files for training.\n",
      "Found 489 files belonging to 5 classes.\n",
      "Using 97 files for validation.\n",
      "['A-Chord', 'Bm-Chord', 'C-Chord', 'D-Chord', 'G-Chord']\n"
     ]
    }
   ],
   "source": [
    "guitarChordDetectModel.output_images_path\n",
    "train_dataset, test_dataset = guitarChordDetectModel.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
